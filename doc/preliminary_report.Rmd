---
title: "#zerowaste tweets - preliminary report"
output:
  github_document
---

> Martin Fridrich 01/2020 

This notebook aims to explore the #zerowaste data and provide a reader with several insights. It will serve as a baseline for downstream transformation & modeling procedures. The structure of the analysis is as follows:

1 [Housekeepin']  
2 [Exploratory data analysis]  
&nbsp;&nbsp;2.1 [Computing characteristics]  
&nbsp;&nbsp;2.2 [Visualizations]  
3 [Next steps]

## Housekeepin'

```{r setup, include=F}
to_install = c("data.table", "bit64", "tidyverse", "janitor", "devtools")

for (pi in 1:length(to_install)){
  if(!to_install[pi] %in% rownames(installed.packages())){
    install.packages(to_install[pi])}
}
```

In the opening section, we digest raw CSV files & union them into the resulting `data.frame`. Moreover, we sanitize the column names and present the overall structure of the dataset.

```{r message=F, warning=F}
library(tidyverse)
data_dir = "..//data//"
csv_to_load = paste0(data_dir, list.files(data_dir, pattern=".csv"))
csv_ls = list()

for (fi in 1:length(csv_to_load)){
  csv_ls[[fi]] = data.table::fread(csv_to_load[fi],
    data.table=F, na.strings="", nThread=4)}

raw_tweets = data.table::rbindlist(csv_ls)
raw_tweets = raw_tweets %>% janitor::clean_names()
as_tibble(head(raw_tweets))
```

The data consists of `r nrow(raw_tweets)` rows and `r ncol(raw_tweets)` columns. It appears to be an extended version of the export with more details regarding both the tweets (interactions, language, links between the tweets, location, device, etc.) & accounts (verification, language, location, etc.). The columns appear to be loaded in the correct format except for the `Date`. This allows us to incorporate much broader perspectives of the modeling.

```{r echo=F, results='hide'}
rm(list=setdiff(ls(),"raw_tweets"))
gc()
```

## Exploratory data analysis

Within this section, we extract & examine selected properties of the tweets considering both texts and covariates.

```{r cache=F}
# users
nu_users = length(unique(raw_tweets$user_id)) #on ids
nu_verified = length(unique(raw_tweets$user_id[raw_tweets$user_verified]))
# tweets
nid_tweets = length(unique(raw_tweets$id)) #on ids
nu_tweets = length(unique(raw_tweets$text)) #on texts
nid_retweets = sum(!is.na(raw_tweets$in_retweet_to_id))
nu_retweets = sum(grepl("RT @",raw_tweets$text))
```

We start with fundamental characteristics. However, we identify `r nu_users` unique user accounts; only `r nu_verified` of them are verified. We see `r nid_tweets` of unique tweets based on provided `id`, although `r nu_tweets` distinct tweets are based on the text itself. Similarly, we observe `r nid_retweets` retweets based on reference, but `r nu_tweets` based on naive RT detection. We see there is a slight disproportion in those metrics.

### Computing characteristics

In the next code chunk, we construct base objects describing the frequency distribution of the data concerning texts, tokens, covariates, etc.

```{r cache=T}
# texts
tweets = ifelse(raw_tweets$text_truncated,
                raw_tweets$text_full,raw_tweets$text)
n_chars = sapply(tweets, nchar)
names(n_chars) = NULL
n_tokens = stringr::str_count(tweets, "\\w+")
names(n_tokens) = NULL
n_tags = stringr::str_count(tweets, "#\\w+")
names(n_tags) = NULL

# top tokens
tokens = tweets %>%
  paste0(collapse = " ") %>%
  stringr::str_extract_all("\\w+") %>% unlist()
tab_tokens = table(tokens)

# top tags
tags = raw_tweets$hashtags %>%
  paste0(collapse = ",") %>% 
  strsplit(",") %>% unlist()
tab_tags = table(tags[nchar(tags)>0])

# languages
n_langs = table(raw_tweets$lang)

# datetime
cleaned_datetime = strptime(gsub("\\+0000","",raw_tweets$date),
    format="%a %b %d %H:%M:%S %Y")

# users
users = raw_tweets %>%
          group_by(user_id) %>%
          summarise(is_verified=max(user_verified),
                    n_tweets=n_distinct(text))
n_tweets = users$n_tweets

# devices
tab_devices = sapply(raw_tweets$source,
  function(x)ifelse(is.na(x),"NA", xml2::xml_text(xml2::read_html(x))))
tab_devices = table(tab_devices)

# locations
tab_locations = sapply(raw_tweets$derived_location,
  function(x)ifelse(is.na(x),"NA", tail(strsplit(x,", ")[[1]],1)))
tab_locations = table(tab_locations)
```

### Visualizations

Here, we visualize the probability density/distributions computed in the preceding code chunk. Firstly, we peek at distributions describing the properties of the tweet text & tags.

```{r char_tok_tag_histograms, fig.width=15, fig.height=6}
par(mfrow=c(1,3))
hist(n_chars,
     main="no of characters per tweet",
     xlab="character count",
     cex.main=1.3,
     cex.axis=1.3,
     cex.lab=1.3)
hist(n_tokens,
     main="no of tokens per tweet",
     xlab="token count",
     cex.main=1.3,
     cex.axis=1.3,
     cex.lab=1.3)
hist(n_tags,
     main="no of tags per tweet",
     xlab="tag count",
     cex.main=1.3,
     cex.axis=1.3,
     cex.lab=1.3)
```

From the left plot, we can see that approx. half of the tweets are shorter than 140 chars; however, some extended texts are almost 1000 char long. The middle plot shows that 75 % of the tweets consist of 25 words or less. Similarly, the right plot displays that 75 % of the tweets employ less than four hashtags. We recommend utilizing both texts & tweets to achieve acceptable performance in downstream steps considering the frequency distributions.

```{r tok_tag_bars, fig.width=15, fig.height=6}
par(mfrow=c(1,2), mar=c(4,7.5,2,2))
barplot(rev(log10(sort(tab_tokens, decreasing=T)[1:20])),
        xlim=c(0,6),
        horiz = T, las=1,
        main="the most frequent tokens",
        xlab="log10 token count")
barplot(rev(log10(sort(tab_tags, decreasing=T)[1:20])),
        xlim=c(0,6),
        horiz = T, las=1,
        main="the most frequent hashtags",
        xlab="log10 tag count")
```

In the first plot (left), we can see the 20 most popular hashtags; most of the tags are relevant to the domain at hand. However, some of them, such as `UnitedKingdom`, `Singapore` or `Mexico` suggest local diffusion. In the second plot (right), one can see the most common word tokens. Interestingly, the first three places are occupied by artifacts from web addresses. It becomes evident that both tags & texts need polishing.

```{r}
round(sort(n_langs, decreasing=T)[1:10]/nrow(raw_tweets),3)*100
```

Most of the observed texts are written in English. Thus, only ~ 70 % of the data can be utilized as intended (i.e., tags + texts hybrid topic model).

```{r year_hour_histograms, fig.width=15, fig.height=6}
par(mfrow=c(1,2))
hist(lubridate::year(cleaned_datetime),
     main="no of tweets over years",
    xlab="year")
hist(lubridate::hour(cleaned_datetime),
     main="no of tweets over day-hour",
     xlab="day-hour",
     breaks=24)
```

On the left, one observes a steady incline until 2019, a decline since. On the right, we can see the frequency distribution of the data during the day. The first peak (00) still breaks out from a somewhat smooth distribution.

```{r user_histograms, fig.width=15, fig.height=6}
par(mfrow=c(1,2))
hist(log10(users$n_tweets[users$is_verified==0]),
     main="no of tweets per regular user",
     xlab="log10 tweet count")
hist(log10(users$n_tweets[users$is_verified==1]),
     main="no of tweets per verified user",
     xlab="log10 tweet count",
     xlim=c(0,5))
```

The plots above suggest that an average number of tweets by regular or verified users might be drawn from the same underlying probability distribution.

```{r interaction_histograms, fig.width=15, fig.height=6}
par(mfrow=c(1,3))
hist(log10(raw_tweets$reply_count),
     main="no of replies per tweet",
     xlab="log10 reply count",
     xlim=c(0,4),
     cex.main=1.3,
     cex.axis=1.3,
     cex.lab=1.3)
hist(log10(raw_tweets$favorite_count),
     main="no of favorites per tweet",
     xlab="log10 heart count",
     xlim=c(0,4),
     cex.main=1.3,
     cex.axis=1.3,
     cex.lab=1.3)
hist(log10(raw_tweets$quote_count),
     main="no of retweets per tweet",
     xlab="log10 quote/retweet count",
     xlim=c(0,4),
     cex.main=1.3,
     cex.axis=1.3,
     cex.lab=1.3)
```

The frequency distribution of user-interactions with the content are all strongly right-skewed. We consider including a total number of interactions as a somewhat straightforward measure of relevance.

```{r source_state_bars, fig.width=15, fig.height=6}
par(mfrow=c(1,2), mar=c(4,8,2,2))
barplot(rev(log10(sort(tab_devices, decreasing=T)[1:20])),
        xlim=c(0,6),
        horiz = T, las=1,
        main="the most frequent tweet sources",
        xlab="log10 tweet count")
barplot(rev(log10(sort(tab_locations, decreasing=T)[1:20])),
        xlim=c(0,6),
        horiz = T, las=1,
        main="the most frequent states",
        xlab="log10 tweet count")
```

On the left, we see that even though most popular tweet sources are organic, there is a considerable amount of marketing automation apps. On the right, we see approximate locations of the tweets on a state level. Both perspectives might help us in further efforts concerning the relevancy of the analysis's tweets & scope.

```{r}
# data integrity
missing_retweets = 1-sum(raw_tweets$in_retweet_to_id %in% raw_tweets$id)/
  sum(!is.na(raw_tweets$in_retweet_to_id))
missing_quotes = 1-sum(raw_tweets$in_quote_to_id %in% raw_tweets$id)/
  sum(!is.na(raw_tweets$in_quote_to_id))
```

Lastly, we check on data-integrity of the export in terms on the retweets & quotes. We see that out of `r sum(!is.na(raw_tweets$in_retweet_to_id))` retweets, there is `r round(missing_retweets*100,2)` % of them missing. Similarly, out of `r sum(!is.na(raw_tweets$in_quote_to_id))` quotes, `r round(missing_quotes*100,2)` % of them are missing.

## Next steps

**General**
 
 * Re-iterate on the project scope & timeline.
 * Re-iterate on the priors.
 * Determine tweet relevance.

**Dataset**

We would like to have detailed info regarding the data generation process. Some of the issues from the first dataset still prevail. However, addressing the problems have to be align with the project scope (e.g. page-ranking users might not make any sense with the dataset at hand, etc.).

**Preprocessing**

Filtering based on account entities & different parameters for selection should be examined. This is necessary in order to eliminate irrelevant observations & features. Moreover, we propose to employ hybrid model based on texts, tags & covariates. Thus, respective processing pipelines have to be designed and developed.

> Martin Fridrich 01/2020
